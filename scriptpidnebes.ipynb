{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction\n",
    "\n",
    "This notebook aims to predict heart disease using various machine learning techniques. The dataset includes several medical attributes that contribute to heart disease outcomes. We will perform data preprocessing, explore the creation of interaction terms, encode categorical variables, and apply machine learning models to predict the presence of heart disease.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "import category_encoders as ce\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBRFClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Preliminary Exploration\n",
    "\n",
    "We start by loading the training and test datasets and conduct a brief exploration to understand the structure and type of data we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPainType</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>RestingECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExerciseAngina</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>ST_Slope</th>\n",
       "      <th>HeartDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>563</td>\n",
       "      <td>55</td>\n",
       "      <td>M</td>\n",
       "      <td>ASY</td>\n",
       "      <td>135</td>\n",
       "      <td>204</td>\n",
       "      <td>1</td>\n",
       "      <td>ST</td>\n",
       "      <td>126</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>884</td>\n",
       "      <td>67</td>\n",
       "      <td>M</td>\n",
       "      <td>ASY</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>LVH</td>\n",
       "      <td>108</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>352</td>\n",
       "      <td>56</td>\n",
       "      <td>M</td>\n",
       "      <td>ASY</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ST</td>\n",
       "      <td>100</td>\n",
       "      <td>Y</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Down</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>694</td>\n",
       "      <td>56</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>178</td>\n",
       "      <td>N</td>\n",
       "      <td>0.8</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>491</td>\n",
       "      <td>75</td>\n",
       "      <td>M</td>\n",
       "      <td>ASY</td>\n",
       "      <td>170</td>\n",
       "      <td>203</td>\n",
       "      <td>1</td>\n",
       "      <td>ST</td>\n",
       "      <td>108</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  \\\n",
       "0  563   55   M           ASY        135          204          1         ST   \n",
       "1  884   67   M           ASY        160          286          0        LVH   \n",
       "2  352   56   M           ASY        120            0          0         ST   \n",
       "3  694   56   M           ATA        120          236          0     Normal   \n",
       "4  491   75   M           ASY        170          203          1         ST   \n",
       "\n",
       "   MaxHR ExerciseAngina  Oldpeak ST_Slope  HeartDisease  \n",
       "0    126              Y      1.1     Flat             1  \n",
       "1    108              Y      1.5     Flat             1  \n",
       "2    100              Y     -1.0     Down             1  \n",
       "3    178              N      0.8       Up             0  \n",
       "4    108              N      0.0     Flat             1  "
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "train_data = pd.read_csv('train_heart.csv', sep=',')\n",
    "test_data = pd.read_csv('test_heart.csv', sep=',')\n",
    "\n",
    "# Display the first few rows of the training data\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "In this section, we address missing values, create interaction terms, encode categorical variables, and scale the features to prepare our data for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we address potential outliers and missing values in the dataset. Specifically, we focus on two important variables: `Cholesterol` and `RestingBP` (Resting Blood Pressure). Zero values in these variables are not plausible and likely indicate missing or incorrectly recorded data. We replace these zero values with the median value of the respective variable, which is a robust measure of central tendency that is less sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\germa\\AppData\\Local\\Temp\\ipykernel_6044\\793061652.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_data['Cholesterol'].replace(0, chol_median, inplace=True)\n",
      "C:\\Users\\germa\\AppData\\Local\\Temp\\ipykernel_6044\\793061652.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_data['RestingBP'].replace(0, resting_median, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Filter values for Cholesterol and RestingBP\n",
    "chol_median = train_data.loc[train_data['Cholesterol'] != 0, 'Cholesterol'].median()\n",
    "train_data['Cholesterol'].replace(0, chol_median, inplace=True)\n",
    "\n",
    "resting_median = train_data.loc[train_data['RestingBP'] != 0, 'RestingBP'].median()\n",
    "train_data['RestingBP'].replace(0, resting_median, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Interaction Term\n",
    "\n",
    "Interaction terms can capture the effect of two or more variables acting together on the target variable. Here, we create an interaction term between `Age` and `Cholesterol`, hypothesizing that the combination of these variables might have a different effect on heart disease risk compared to considering them individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Age_Chol_Interact'] = train_data['Age'] * train_data['Cholesterol']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode Categorical Variables\n",
    "\n",
    "Many machine learning models cannot handle categorical variables unless they are converted into numerical values. We use ordinal encoding to transform these variables into a numerical format while preserving the order of categories when applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = train_data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Encode categorical variables\n",
    "encoder = ce.OrdinalEncoder(cols=categorical_columns)\n",
    "X_encoded = encoder.fit_transform(train_data.drop(['id', 'HeartDisease'], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Polynomial Features\n",
    "\n",
    "Polynomial features are generated by raising existing features to an exponent. This technique helps to capture interactions between the original features by adding squared or higher order terms of the features. It can uncover complex relationships between features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and apply PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale Features\n",
    "Feature scaling is essential to normalize the range of independent variables or features of data. In the absence of scaling, the model might get biased toward high values. We use StandardScaler for this purpose, which standardizes features by removing the mean and scaling to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step in data preprocessing is splitting the dataset into training and test sets. This allows us to train our model on one subset of the data and then test it on a separate subset to evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable\n",
    "y = train_data['HeartDisease']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data preprocessing completed, we're now ready to move on to the next steps: model training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering and Model Preparation\n",
    "\n",
    "In this step, we focus on optimizing the model's performance through hyperparameter tuning. Hyperparameters are the settings that can be adjusted to control the model's learning process. Tuning these parameters helps in finding the best version of the model that can predict more accurately.\n",
    "\n",
    "For our Decision Tree classifier, we will explore a range of hyperparameters to find the best combination that yields the highest accuracy. This process is facilitated by `GridSearchCV`, a tool that performs exhaustive search over specified parameter values for an estimator.\n",
    "\n",
    "#### Hyperparameter Tuning with GridSearchCV\n",
    "\n",
    "The parameters we are tuning include:\n",
    "- `max_depth`: The maximum depth of the tree.\n",
    "- `min_samples_split`: The minimum number of samples required to split an internal node.\n",
    "- `min_samples_leaf`: The minimum number of samples required to be at a leaf node.\n",
    "- `ccp_alpha`: Complexity parameter used for Minimal Cost-Complexity Pruning.\n",
    "- `splitter`: The strategy used to choose the split at each node.\n",
    "- `class_weight`: Weights associated with classes.\n",
    "\n",
    "The goal is to find the optimal settings that lead to the best model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1296 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found for decisiontree: {'ccp_alpha': 0.005, 'class_weight': 'balanced', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'random'}\n",
      "Best parameters found for randomforest: {'colsample_bynode': 0.1, 'max_depth': 10, 'n_estimators': 200, 'subsample': 0.9}\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid_extended = {\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'ccp_alpha': [0.0, 0.001, 0.005, 0.01, 0.05, 0.1],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# Updating the GridSearchCV setup\n",
    "grid_search_extended = GridSearchCV(\n",
    "    estimator=DecisionTreeClassifier(random_state=42),\n",
    "    param_grid=param_grid_extended,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "# RandomForest GridSearchCV setup\n",
    "param_grid_randomforest = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20, 30],  # Maximum depth of the tree\n",
    "    'subsample': [0.2, 0.5, 0.7, 0.9],\n",
    "    'colsample_bynode': [0.1, 0.2, 0.5, 0.7],\n",
    "}\n",
    "\n",
    "grid_search_randomforest = GridSearchCV(estimator=XGBRFClassifier(), \n",
    "                                        param_grid=param_grid_randomforest,\n",
    "                                        cv=5, \n",
    "                                        scoring='accuracy', \n",
    "                                        n_jobs=-1\n",
    "                                        )\n",
    "\n",
    "# Fit the model with grid search\n",
    "grid_search_extended.fit(X_train, y_train)\n",
    "\n",
    "grid_search_randomforest.fit(X_train, y_train)\n",
    "\n",
    "# Access best_params_ to see the best set of parameters found by GridSearchCV\n",
    "best_params = grid_search_extended.best_params_\n",
    "print(f\"Best parameters found for decisiontree: {best_params}\")\n",
    "\n",
    "best_params_random = grid_search_randomforest.best_params_\n",
    "print(f\"Best parameters found for randomforest: {best_params_random}\")\n",
    "\n",
    "# Use the best estimator directly\n",
    "base_est = grid_search_extended.best_estimator_\n",
    "base_est_rf = grid_search_randomforest.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the best hyperparameters identified, we can now proceed to use this optimally tuned Decision Tree as the base estimator for our AdaBoost classifier. This ensures that our ensemble model starts with a strong foundation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training with AdaBoost\n",
    "\n",
    "After tuning our decision tree model to find the optimal hyperparameters, we proceed to the next significant phaseâ€”model training. In this step, we employ the AdaBoost algorithm, an ensemble method that combines multiple weak learners (in this case, decision trees) to create a strong classifier. By adjusting the weights of incorrectly classified instances, AdaBoost focuses on the hard cases, thereby improving the model's performance on the training data.\n",
    "\n",
    "### Training the AdaBoost Model\n",
    "\n",
    "AdaBoost works by iteratively adding models (in this case, decision trees) that correct the mistakes of the models already added to the ensemble. We specify the number of trees to be used (`n_estimators`) and use the best decision tree model (`base_est`) identified in the previous step as our base estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\germa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Decision Tree Accuracy: 0.8393782383419689\n",
      "XGBoost Random Forest Accuracy: 0.8549222797927462\n"
     ]
    }
   ],
   "source": [
    "# Define the AdaBoost ensemble model and XGB RandomForest using the best decision tree estimator\n",
    "ada_boost_clf = AdaBoostClassifier(estimator=base_est, n_estimators=1000, random_state=42)\n",
    "# Train the AdaBoost and XGBRF model\n",
    "ada_boost_clf.fit(X_train, y_train)\n",
    "base_est_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_ada = ada_boost_clf.predict(X_test)\n",
    "y_pred_xgb_rf = base_est_rf.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy_ada = accuracy_score(y_test, y_pred_ada)\n",
    "accuracy_xgb_rf = accuracy_score(y_test, y_pred_xgb_rf)\n",
    "\n",
    "print(f'AdaBoost Decision Tree Accuracy: {accuracy_ada}')\n",
    "print(f'XGBoost Random Forest Accuracy: {accuracy_xgb_rf}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting Prediction Threshold\n",
    "\n",
    "A key aspect of model evaluation involves adjusting the prediction threshold. This adjustment can help in achieving a balance between sensitivity and specificity, based on the prediction problem's requirements. Here, we experiment with a new threshold to see its impact on the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted AdaBoost Decision Tree Accuracy: 0.8393782383419689\n"
     ]
    }
   ],
   "source": [
    "# Get probabilities of the positive class\n",
    "y_probs = ada_boost_clf.predict_proba(X_test)[:, 1]\n",
    "# Define a new threshold\n",
    "new_threshold = 0.4  # Example threshold\n",
    "\n",
    "# Apply new threshold to get the adjusted predictions\n",
    "y_pred_adjusted = np.where(y_probs > new_threshold, 1, 0)\n",
    "\n",
    "# Calculate and print the adjusted accuracy\n",
    "accuracy_adjusted = accuracy_score(y_test, y_pred_adjusted)\n",
    "print(f'Adjusted AdaBoost Decision Tree Accuracy: {accuracy_adjusted}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "\n",
    "The final step in model training is to evaluate the performance of our classifier. We do this by calculating the confusion matrix and classification report, which provide insights into the accuracy, precision, recall, and F1-score of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[64 22]\n",
      " [ 9 98]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.74      0.81        86\n",
      "           1       0.82      0.92      0.86       107\n",
      "\n",
      "    accuracy                           0.84       193\n",
      "   macro avg       0.85      0.83      0.83       193\n",
      "weighted avg       0.84      0.84      0.84       193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print confusion matrix and classification report\n",
    "cm = confusion_matrix(y_test, y_pred_adjusted)\n",
    "cr = classification_report(y_test, y_pred_adjusted)\n",
    "\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"\\nClassification Report:\\n\", cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through these evaluation metrics, we can assess the model's ability to classify instances accurately and understand its strengths and weaknesses in predicting heart disease. This critical analysis guides us in making any necessary adjustments to improve model performance further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on New Data\n",
    "\n",
    "Having trained and evaluated our model on a training dataset, the next critical step is to test its performance on new, unseen data. This step is essential for understanding how well our model generalizes to data it has not encountered before. We follow the same data preprocessing steps for the new dataset as we did for the training dataset. Finally, we use our trained model to make predictions on this new data and prepare a submission file with these predictions.\n",
    "\n",
    "#### Preparing the Test Data\n",
    "\n",
    "The new data must be preprocessed to match the format expected by the model. This involves filtering out missing or implausible values, creating interaction terms, encoding categorical variables, adding polynomial features, and scaling the features, just as we did with the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\germa\\AppData\\Local\\Temp\\ipykernel_6044\\323765008.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test['Cholesterol'].replace(0, chol_median, inplace=True)\n",
      "C:\\Users\\germa\\AppData\\Local\\Temp\\ipykernel_6044\\323765008.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test['RestingBP'].replace(0, resting_median, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Import the test dataset\n",
    "test = pd.read_csv('test_heart.csv', sep=',')\n",
    "\n",
    "# Apply the same preprocessing steps to the test data\n",
    "# Filter out missing or implausible values for Cholesterol and RestingBP\n",
    "chol_median = test.loc[test['Cholesterol'] != 0, 'Cholesterol'].median()\n",
    "test['Cholesterol'].replace(0, chol_median, inplace=True)\n",
    "\n",
    "resting_median = test.loc[test['RestingBP'] != 0, 'RestingBP'].median()\n",
    "test['RestingBP'].replace(0, resting_median, inplace=True)\n",
    "\n",
    "# Create interaction term\n",
    "test['Age_Chol_Interact'] = test['Age'] * test['Cholesterol']\n",
    "\n",
    "# Prepare the test data\n",
    "X_new = test.drop(['id'], axis=1)\n",
    "\n",
    "# Encode and transform the test data using the same encoder and transformers\n",
    "X_new_encoded = encoder.transform(X_new)\n",
    "X_new_poly = poly.transform(X_new_encoded)\n",
    "X_new_scaled = scaler.transform(X_new_poly)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making Predictions and Adjusting Threshold\n",
    "\n",
    "With the test data prepared, we predict the likelihood of heart disease. We then apply a chosen threshold to these predictions to classify each instance as either having heart disease or not. This threshold is adjustable based on the specific requirements of the task or the desired balance between sensitivity and specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for the test data\n",
    "y_new_probs = base_est_rf.predict(X_new_scaled)\n",
    "\n",
    "# Apply the chosen threshold to classify each instance\n",
    "# chosen_threshold = 0.4  # This threshold can be adjusted\n",
    "# y_new_pred_adjusted = np.where(y_new_probs > chosen_threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the Submission File\n",
    "\n",
    "Finally, we prepare a submission file that includes the predictions for the new data. This file typically contains an ID for each instance and the corresponding prediction. This step is especially relevant in competition or operational settings where predictions need to be shared or deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame for submission\n",
    "id_to_prediction_df = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'HeartDisease': y_new_probs\n",
    "})\n",
    "\n",
    "# Output the submission file\n",
    "file_name = 'submission.csv'\n",
    "id_to_prediction_df.to_csv(file_name, index=False)\n",
    "\n",
    "print(f\"File saved as {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step concludes our process of developing a model to predict heart disease. By carefully preparing the test data, making informed predictions, and generating a submission file, we have applied our model to new data, showcasing its potential for real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "# import category_encoders as ce\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# import sklearn\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# # IMPORT CSV\n",
    "# hearts = pd.read_csv('train_heart.csv', sep=',')\n",
    "\n",
    "# # FILTER VALUES FOR CHOLESTEROL AND RESTINGBP\n",
    "# chol_median = hearts.loc[hearts['Cholesterol'] != 0, 'Cholesterol'].median()\n",
    "# hearts['Cholesterol'].replace(0, chol_median, inplace=True)\n",
    "# resting_median = hearts.loc[hearts['RestingBP'] != 0, 'RestingBP'].median()\n",
    "# hearts['RestingBP'].replace(0, resting_median, inplace=True)\n",
    "\n",
    "# # CREATE INTERACTION TERM\n",
    "# hearts['Age_Chol_Interact'] = hearts['Age'] * hearts['Cholesterol']\n",
    "\n",
    "# # DROP UNNECESSARY COLUMNS AND FIND y\n",
    "# X = hearts.drop(['id', 'HeartDisease'], axis=1)\n",
    "# y = hearts['HeartDisease']\n",
    "\n",
    "# # ENCODE CATEGORICAL VARIABLES\n",
    "# encoder = ce.OrdinalEncoder(cols=X.select_dtypes(include=['object']).columns)\n",
    "# X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "# # ADD POLYNOMIAL FEATURES\n",
    "# poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "# X_poly = poly.fit_transform(X_encoded)\n",
    "\n",
    "# # SCALE FEATURES\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X_poly)\n",
    "\n",
    "# # DATA SPLITTING\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# # Define the parameter grid to search\n",
    "# param_grid_extended = {\n",
    "#     'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "#     'ccp_alpha': [0.0, 0.001, 0.005, 0.01, 0.05, 0.1],\n",
    "#     'splitter': ['best', 'random'],\n",
    "#     'class_weight': [None, 'balanced']\n",
    "# }\n",
    "\n",
    "# # Updating the GridSearchCV setup\n",
    "# grid_search_extended = GridSearchCV(estimator=DecisionTreeClassifier(random_state=42),\n",
    "#                                     param_grid=param_grid_extended,\n",
    "#                                     cv=5,\n",
    "#                                     scoring='accuracy',\n",
    "#                                     verbose=1,\n",
    "#                                     n_jobs=-1)\n",
    "\n",
    "# # Fit the model with grid search\n",
    "# grid_search_extended.fit(X_train, y_train)\n",
    "\n",
    "# # Now you can access best_params_\n",
    "# base_est = grid_search_extended.best_estimator_  # Use the best estimator directly\n",
    "\n",
    "# # Define the AdaBoost ensemble model using the best decision tree estimator\n",
    "# ada_boost_clf = AdaBoostClassifier(estimator=base_est, n_estimators=1000, random_state=42)\n",
    "\n",
    "# # Train the AdaBoost model\n",
    "# ada_boost_clf.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the test set\n",
    "# y_pred_ada = ada_boost_clf.predict(X_test)\n",
    "\n",
    "# # Calculate and print the accuracy\n",
    "# accuracy_ada = accuracy_score(y_test, y_pred_ada)\n",
    "# print(f'AdaBoost Decision Tree Accuracy: {accuracy_ada}')\n",
    "\n",
    "# # Get probabilities of the positive class\n",
    "# y_probs = ada_boost_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# # Define a new threshold\n",
    "# new_threshold = 0.4  # Example threshold\n",
    "\n",
    "# # Apply new threshold to get the adjusted predictions\n",
    "# y_pred_adjusted = np.where(y_probs > new_threshold, 1, 0)\n",
    "\n",
    "# # Calculate and print the adjusted accuracy\n",
    "# accuracy_adjusted = accuracy_score(y_test, y_pred_adjusted)\n",
    "# print(f'Adjusted AdaBoost Decision Tree Accuracy: {accuracy_adjusted}')\n",
    "\n",
    "# # Print confusion matrix and classification report\n",
    "# print(confusion_matrix(y_test, y_pred_adjusted))\n",
    "# print(classification_report(y_test, y_pred_adjusted))\n",
    "# # TESTING PART\n",
    "# # IMPORT CSV\n",
    "# test = pd.read_csv('test_heart.csv', sep=',')\n",
    "\n",
    "# # FILTER VALUES FOR CHOLESTEROL AND RESTINGBP\n",
    "# chol_median = test.loc[test['Cholesterol'] != 0, 'Cholesterol'].median()\n",
    "# test['Cholesterol'].replace(0, chol_median, inplace=True)\n",
    "\n",
    "# resting_median = test.loc[test['RestingBP'] != 0, 'RestingBP'].median()\n",
    "# test['RestingBP'].replace(0, resting_median, inplace=True)\n",
    "\n",
    "# # CREATE INTERACTION TERM\n",
    "# test['Age_Chol_Interact'] = test['Age'] * test['Cholesterol']\n",
    "\n",
    "# # Prepare the test data following the same steps as for the training data\n",
    "# X_new = test.drop(['id'], axis=1)\n",
    "\n",
    "# # ENCODE X using the same encoder\n",
    "# X_new_encoded = encoder.transform(X_new)\n",
    "\n",
    "# # ADD POLYNOMIAL FEATURES AND SCALE using the same transformers\n",
    "# X_new_poly = poly.transform(X_new_encoded)\n",
    "# X_new_scaled = scaler.transform(X_new_poly)\n",
    "\n",
    "# # Predict probabilities for the test data\n",
    "# y_new_probs = ada_boost_clf.predict_proba(X_new_scaled)[:, 1]\n",
    "\n",
    "# # Define the chosen threshold for decision-making\n",
    "# chosen_threshold = 0.4  # Adjust based on your analysis and preference\n",
    "\n",
    "# # Apply the threshold to decide the predicted class\n",
    "# y_new_pred_adjusted = np.where(y_new_probs > chosen_threshold, 1, 0)\n",
    "\n",
    "# # CREATE PANDAS DATAFRAME FOR SUBMISSION\n",
    "# id_to_prediction_df = pd.DataFrame({\n",
    "#     'id': test['id'],\n",
    "#     'HeartDisease': y_new_pred_adjusted\n",
    "# })\n",
    "# # OUTPUT TO CSV\n",
    "# file_name = './submission.csv'\n",
    "# id_to_prediction_df.to_csv(file_name, index=False)\n",
    "\n",
    "# print(f\"File saved as {file_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
