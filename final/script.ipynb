{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\germa\\AppData\\Local\\Temp\\ipykernel_28096\\1703452496.py:115: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_data['Cholesterol'].replace(0, chol_median, inplace=True)\n",
      "C:\\Users\\germa\\AppData\\Local\\Temp\\ipykernel_28096\\1703452496.py:118: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_data['RestingBP'].replace(0, resting_median, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1296 candidates, totalling 6480 fits\n",
      "{'ccp_alpha': 0.005, 'class_weight': 'balanced', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'random'}\n",
      "[[-1.90026337 -0.51601569 -0.80799877 ... -0.91303667 -1.16081952\n",
      "  -1.05759192]\n",
      " [ 0.03142589 -0.51601569 -0.80799877 ... -0.91303667 -0.99289634\n",
      "  -0.64457795]\n",
      " [-0.39783839  1.93792558 -0.80799877 ...  1.18887114  1.069128\n",
      "  -0.19075392]\n",
      " ...\n",
      " [-0.50515446 -0.51601569 -0.80799877 ...  1.18887114  0.24869691\n",
      "  -0.95515186]\n",
      " [-1.25636695 -0.51601569  1.27527517 ... -0.91303667 -1.07685793\n",
      "  -0.86311824]\n",
      " [ 0.67532231 -0.51601569 -0.80799877 ...  1.18887114  1.62887192\n",
      "   0.47728362]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\germa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with AdaBoost 0.8393782383419689\n",
      "Accuracy with RandomForest 0.8652849740932642\n",
      "Accuracy with LogisticRegression 0.8549222797927462\n",
      "'numpy.ndarray' object has no attribute 'compare'\n",
      "'numpy.ndarray' object has no attribute 'compare'\n",
      "'numpy.ndarray' object has no attribute 'compare'\n",
      "[1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0\n",
      " 1 0 1 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 1 1 1 0\n",
      " 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1\n",
      " 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0\n",
      " 1 0 1 1 1 1 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\germa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier Accuracy: 0.8601036269430051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\germa\\AppData\\Local\\Temp\\ipykernel_28096\\1703452496.py:245: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_data['Cholesterol'].replace(0, chol_median, inplace=True)\n",
      "C:\\Users\\germa\\AppData\\Local\\Temp\\ipykernel_28096\\1703452496.py:248: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_data['RestingBP'].replace(0, resting_median, inplace=True)\n",
      "c:\\Users\\germa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but AdaBoostClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 12 features, but AdaBoostClassifier is expecting 90 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 256\u001b[0m\n\u001b[0;32m    252\u001b[0m X_new \u001b[38;5;241m=\u001b[39m test_data\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    253\u001b[0m X_new_encoded \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mtransform(X_new)\n\u001b[1;32m--> 256\u001b[0m y_new_pred_ada \u001b[38;5;241m=\u001b[39m \u001b[43mada_boost_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_new_encoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m y_new_pred_rf \u001b[38;5;241m=\u001b[39m rf_clf\u001b[38;5;241m.\u001b[39mpredict(X_new_encoded)\n\u001b[0;32m    258\u001b[0m y_new_pred_log_reg \u001b[38;5;241m=\u001b[39m log_reg\u001b[38;5;241m.\u001b[39mpredict(X_new_encoded)\n",
      "File \u001b[1;32mc:\\Users\\germa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:719\u001b[0m, in \u001b[0;36mAdaBoostClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    703\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predict classes for X.\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \n\u001b[0;32m    705\u001b[0m \u001b[38;5;124;03m    The predicted class of an input sample is computed as the weighted mean\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 719\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(pred \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\germa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:780\u001b[0m, in \u001b[0;36mAdaBoostClassifier.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the decision function of ``X``.\u001b[39;00m\n\u001b[0;32m    762\u001b[0m \n\u001b[0;32m    763\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;124;03m    class in ``classes_``, respectively.\u001b[39;00m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    779\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 780\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_X\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_\n\u001b[0;32m    783\u001b[0m classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[:, np\u001b[38;5;241m.\u001b[39mnewaxis]\n",
      "File \u001b[1;32mc:\\Users\\germa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:98\u001b[0m, in \u001b[0;36mBaseWeightBoosting._check_X\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_X\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# Only called to validate X in non-fit methods, therefore reset=False\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\germa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:654\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 654\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\germa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:443\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 12 features, but AdaBoostClassifier is expecting 90 features as input."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "import category_encoders as ce\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sweetviz as sv\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, f1_score,\n",
    "                             precision_score, recall_score, accuracy_score,\n",
    "                             roc_auc_score, roc_curve)\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "\n",
    "# PLOTTING FUNCTIONS\n",
    "def plot_precision_recall_curve(y_true, y_scores):\n",
    "    \"\"\"\n",
    "    Plot Precision-Recall curve for the model.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: Actual target values.\n",
    "    - y_scores: Target scores, can either be probability estimates of the positive class.\n",
    "    \"\"\"\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    auc_score = auc(recall, precision)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, label=f'Precision-Recall curve (area = {auc_score:.2f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def performance_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate and print model performance metrics.\"\"\"\n",
    "    # Calculate metrics\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f'F1-Score: {f1:.5f}\\nPrecision: {precision:.5f}\\nRecall: {recall:.5f}\\nAccuracy: {accuracy:.5f}')\n",
    "\n",
    "\n",
    "def roc_curve_plot(y_true, y_probs):\n",
    "    \"\"\"Plot ROC curve based on true labels and predicted probabilities.\"\"\"\n",
    "    roc_auc = roc_auc_score(y_true, y_probs)\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
    "    \n",
    "    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.savefig('roc_curve.png')\n",
    "    plt.show()\n",
    "\n",
    "def confusion_matrix_heatmap(y_true, y_pred):\n",
    "    \"\"\"Generate and display a confusion matrix heatmap.\"\"\"\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.savefig('confusion_matrix_heatmap.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_feature_importance(model, feature_names, top_n_features=20):\n",
    "    \"\"\"\n",
    "    Plot the importance of features in the model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The trained model.\n",
    "    - feature_names: List of names corresponding to the features used by the model.\n",
    "    \"\"\"\n",
    "    # Extract feature importance from the model\n",
    "    importances = model.feature_importances_\n",
    "    \n",
    "    # Sort feature importances in descending order and select the top N\n",
    "    indices = np.argsort(importances)[::-1][:top_n_features]\n",
    "    \n",
    "    # Rearrange feature names so they match the sorted feature importances\n",
    "    names = [feature_names[i] for i in indices]\n",
    "    sorted_importances = importances[indices]\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(\"Top {} Feature Importance\".format(top_n_features))\n",
    "    plt.bar(range(top_n_features), sorted_importances, align='center')\n",
    "    \n",
    "    # Add feature names as x-axis labels\n",
    "    plt.xticks(range(top_n_features), names, rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()  # Adjust layout to make room for the rotated x-axis labels\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_csv('train_heart.csv', sep=',')\n",
    "test_data = pd.read_csv('test_heart.csv', sep=',')\n",
    "\n",
    "# Filter values for Cholesterol and RestingBP\n",
    "chol_median = train_data.loc[train_data['Cholesterol'] != 0, 'Cholesterol'].median()\n",
    "train_data['Cholesterol'].replace(0, chol_median, inplace=True)\n",
    "\n",
    "resting_median = train_data.loc[train_data['RestingBP'] != 0, 'RestingBP'].median()\n",
    "train_data['RestingBP'].replace(0, resting_median, inplace=True)\n",
    "\n",
    "# Create interaction term\n",
    "train_data['Age_Chol_Interact'] = train_data['Age'] * train_data['Cholesterol']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = train_data.select_dtypes(include=['object']).columns\n",
    "encoder = ce.OrdinalEncoder(cols=categorical_columns)\n",
    "X_encoded = encoder.fit_transform(train_data.drop(['id', 'HeartDisease'], axis=1))\n",
    "\n",
    "#poly features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_encoded = poly.fit_transform(X_encoded)\n",
    "poly_feature_names = poly.get_feature_names_out(input_features=encoder.get_feature_names_out())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_encoded = scaler.fit_transform(X_encoded)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "y = train_data['HeartDisease']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Hyperparameter Tuning with GridSearchCV\n",
    "param_grid_ada = {\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'ccp_alpha': [0.0, 0.001, 0.005, 0.01, 0.05, 0.1],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "grid_search_ada = GridSearchCV(\n",
    "    estimator=DecisionTreeClassifier(random_state=42),\n",
    "    param_grid=param_grid_ada,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search_ada.fit(X_train, y_train)\n",
    "\n",
    "best_params_ada = grid_search_ada.best_params_\n",
    "best_est_ada = grid_search_ada.best_estimator_\n",
    "\n",
    "print(best_params_ada)\n",
    "\n",
    "dict_weights = {1:1, 2: 1, 3: 1, 4: 2, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1}\n",
    "print(X_train)\n",
    "\n",
    "# Model Training with AdaBoost\n",
    "ada_boost_clf = AdaBoostClassifier(estimator=best_est_ada, n_estimators=1000, random_state=42)\n",
    "ada_boost_clf.fit(X_train, y_train)\n",
    "y_pred_ada = ada_boost_clf.predict(X_test)\n",
    "accuracy_ada = accuracy_score(y_test, y_pred_ada)\n",
    "print('Accuracy with AdaBoost', accuracy_ada)\n",
    "# Additional models\n",
    "\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100,  random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print('Accuracy with RandomForest', accuracy_rf)\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "accuracy_log = accuracy_score(y_test, y_pred_log_reg)\n",
    "print('Accuracy with LogisticRegression', accuracy_log)\n",
    "\n",
    "\n",
    "#print differences\n",
    "def compare_dfs(*args):\n",
    "    dflist = []\n",
    "    for item in args:\n",
    "        dflist.append(item)\n",
    "    for i in range(len(dflist)):\n",
    "        try:\n",
    "            print(dflist[i].compare(dflist[i+1]))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "compare_dfs(y_pred_ada, y_pred_rf, y_pred_log_reg)\n",
    "\n",
    "# Combine predictions\n",
    "combined_predictions = np.array([y_pred_ada, y_pred_rf, y_pred_log_reg])\n",
    "final_predictions = np.apply_along_axis(lambda x: 1 if np.sum(x) > 1 else 0, axis=0, arr=combined_predictions)\n",
    "print(final_predictions)\n",
    "accuracy_combined = accuracy_score(y_test, final_predictions)\n",
    "\n",
    "\n",
    "# Evaluating the Model\n",
    "cm = confusion_matrix(y_test, final_predictions)\n",
    "cr = classification_report(y_test, final_predictions)\n",
    "\n",
    "\n",
    "\n",
    "# sklearn VotingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "estimators = [\n",
    "    ('ada_boost', ada_boost_clf),\n",
    "    ('random_forest', rf_clf),\n",
    "    ('logistic_regression', log_reg)\n",
    "]\n",
    "\n",
    "# Create the VotingClassifier\n",
    "voting_clf = VotingClassifier(estimators, voting='hard', weights=[1, 2, 1])  # 'hard' for majority voting\n",
    "\n",
    "# Train the VotingClassifier on the training data\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the VotingClassifier\n",
    "y_pred_voting = voting_clf.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy_voting = accuracy_score(y_test, y_pred_voting)\n",
    "print(f'Voting Classifier Accuracy: {accuracy_voting}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Testing on New Data\n",
    "chol_median = test_data.loc[test_data['Cholesterol'] != 0, 'Cholesterol'].median()\n",
    "test_data['Cholesterol'].replace(0, chol_median, inplace=True)\n",
    "\n",
    "resting_median = test_data.loc[test_data['RestingBP'] != 0, 'RestingBP'].median()\n",
    "test_data['RestingBP'].replace(0, resting_median, inplace=True)\n",
    "\n",
    "test_data['Age_Chol_Interact'] = test_data['Age'] * test_data['Cholesterol']\n",
    "\n",
    "X_new = test_data.drop(['id'], axis=1)\n",
    "X_new_encoded = encoder.transform(X_new)\n",
    "\n",
    "\n",
    "y_new_pred_ada = ada_boost_clf.predict(X_new_encoded)\n",
    "y_new_pred_rf = rf_clf.predict(X_new_encoded)\n",
    "y_new_pred_log_reg = log_reg.predict(X_new_encoded)\n",
    "\n",
    "combined_new_predictions = np.array([y_new_pred_ada, y_new_pred_rf, y_new_pred_log_reg])\n",
    "final_new_predictions = np.apply_along_axis(lambda x: 1 if np.sum(x) > 1 else 0, axis=0, arr=combined_new_predictions)\n",
    "\n",
    "final_id_to_prediction_df = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'HeartDisease': final_new_predictions\n",
    "})\n",
    "\n",
    "final_file_name = './final_submission.csv'\n",
    "final_id_to_prediction_df.to_csv(final_file_name, index=False)\n",
    "\n",
    "# Performance Metrics\n",
    "# plot_precision_recall_curve(y_test, y_probs)\n",
    "# performance_metrics(y_test, y_pred_adjusted)\n",
    "# roc_curve_plot(y_test, y_probs)\n",
    "# confusion_matrix_heatmap(y_test, y_pred_adjusted)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
